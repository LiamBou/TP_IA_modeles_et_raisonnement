{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/introHuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace : des modèles entraînés\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) est un site recueillant les modèles IA entraînés, les plus connus (GPT, Bart, Mistral...) comme des plus modestes.\n",
    "Ce site met aussi à disposition des dataset (comme le site kaggle).\n",
    "\n",
    "Un modèle comprend plusieurs éléments. Par exemple, pour le texte, il contiendra un vocabulaire, l'outil de \"tokenisation\", un réseau entraîné, et un outil de restitution du résultat.\n",
    "\n",
    "Les modèles peuvent être téléchargés en local (attention à la taille de certains), ou utilisés en ligne.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Transformers\n",
    "\n",
    "Les transformers sont utilisés pour convertir une entrée (texte par exemple) en entrée assimilable par un ensemble d'outils contenant un réseau de neurones.\n",
    "\n",
    "Il est nécessaire de les télécharger comme une librairie classique :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bouli\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "#téléchargement, un peu long parfois, de la librairie transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#importation en mémoire\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "#importation en mémoire\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## pipeline\n",
    "\n",
    "Un pipeline est une suite d'outils (tokenizer, analyseur, ...) visant un but précis.\n",
    "\n",
    "Par exemple, le pipeline suivant analyse le \"sentiment\" d'un texte :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chargement du \"detecteur de sentiments\" par défaut\n",
    "#la première utilisation prend un peu de temps de téléchargement\n",
    "detecteur_de_sentiments = pipeline(\"sentiment-analysis\")\n",
    "#chargement du \"detecteur de sentiments\"  cardiffnlp\n",
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classement par defaut = \n",
      "I like discover new thing in my courses at UPHF.\n",
      "\t classifier par defaut :  {'label': 'POSITIVE', 'score': 0.9983413219451904}\n",
      "\t classifier cardiff :  {'label': 'positive', 'score': 0.9546298980712891}\n",
      "But I hate writing my practical work reports !\n",
      "\t classifier par defaut :  {'label': 'NEGATIVE', 'score': 0.9973371624946594}\n",
      "\t classifier cardiff :  {'label': 'negative', 'score': 0.9157376289367676}\n",
      "I should better sleep at night rather than in the courses\n",
      "\t classifier par defaut :  {'label': 'NEGATIVE', 'score': 0.9995858073234558}\n",
      "\t classifier cardiff :  {'label': 'neutral', 'score': 0.5100991129875183}\n"
     ]
    }
   ],
   "source": [
    "phrases =     [\n",
    "        \"I like discover new thing in my courses at UPHF.\",\n",
    "        \"But I hate writing my practical work reports !\", \n",
    "        \"I should better sleep at night rather than in the courses\"\n",
    "    ]\n",
    "\n",
    "default_output = detecteur_de_sentiments(phrases)\n",
    "cardiff_output =pipe(phrases)\n",
    "print(\"classement par defaut = \")\n",
    "for i in range(len(phrases)):\n",
    "    print(phrases[i])\n",
    "    print(\"\\t classifier par defaut : \", default_output[i])\n",
    "    print(\"\\t classifier cardiff : \", cardiff_output[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choix du modèle\n",
    "\n",
    "Sur huggingFace, il est assez simple de choisir son modèle :\n",
    "\n",
    "- cliquez en haut au centre sur **Models**\n",
    "- choisissez dans **Natural Language Processing** le traitement désiré (par exemple TextClassification)\n",
    "- choisissez ensuite le modèle voulu, ou plus téléchargé, ou le plus \"liké\"\n",
    "- cliquez sur **</> use in transformers**\n",
    "- vous obtenez ainsi le code pour charger le pipeline (pour ce TP, on choisira \"Use a pipeline as a high-level helper\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Travail à faire\n",
    "\n",
    "Peu de code dans ce TP.\n",
    "\n",
    "1. **Classement** : Prenez le dataset sur le [classement de livres](https://www.kaggle.com/datasets/abireltaief/books-reviews) et tester 3 modèles (dont au moins un basé sur le français) sur les 50 premières données du dataset. Comparez les résultats.\n",
    "\n",
    "2. **Résumé automatique** : Prenez le dataset [cnn_dailymail](https://huggingface.co/datasets/abisee/cnn_dailymail) et testez 3 modèles de résumé sur les 40 premières lignes du dataset. Evaluez par vous même la qualité des résumés.\n",
    "\n",
    "3. **Génération de texte**. Chargez trois modèles générateurs de texte et demandez la recette des panckakes et évaluez les réponses.  \n",
    "   Commencez la recette : `sentences = generator(\"To make pancakes, I need flour\", max_length=30, num_return_sequences=5,  return_full_text=True)`  \n",
    "   (ceci demande la suite du texte, 5 fois, avec des réponses de 30 mots max).\n",
    "\n",
    "4. **Réponse à tout** : Utilisez 3 modèles de _question-answering_ et posez 3 questions sur le texte anglais de votre choix. Comparez.\n",
    "\n",
    "5. **Réponse à tout 2** : Utilisez maintenant des modèles de type _text2text-generation_ pour ces même questions sur ces mêmes textes. Comparez les modèles.\n",
    "\n",
    "6. **Traduction** : Utilisez maintenant des modèles de type _text2text-generation_ pour traduire ces textes en français, et une autre langue de votre choix. Comparez les modèles.\n",
    "\n",
    "**A rendre**, les codes, les questions et réponses. Ainsi votre réflexion sur la confiance envers les modèles existants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Note python : Pour lire un fichier CSV\n",
    "\n",
    "La librairie pandas est recommandée. Voici des exemples d'utilisation :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pda\n",
    "filename = \"REPERTOIRE/VOTRE_FICHIER.CSV\"\n",
    "#chargement d'un fichier CSV, donnée séparées par des virgules, encodage en UTF-8\n",
    "df = pda.read_csv(filename)\n",
    "\n",
    "#chargement d'un fichier CSV, donnée séparées par des tabulations, encodage en ISO-8859-1\n",
    "#df = pda.read_csv(filename, sep='\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération des valeurs d'une colonne dans un tableau : \n",
    "textes = df['reader_review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question-answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemple d'utilisation de \"question-answering\"\n",
    "pipe = pipeline(\"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from when comes artificial intelligence ?  -  {'score': 0.07508504390716553, 'start': 119, 'end': 123, 'answer': '1956'}\n",
      "what is the concept behind artificial intelligence ?  -  {'score': 0.7541843056678772, 'start': 533, 'end': 546, 'answer': 'automated art'}\n"
     ]
    }
   ],
   "source": [
    "context = \"The academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since.[20] Since its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[21] The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[22][23] The tradition of creative automatons has flourished throughout history, exemplified by Maillardet's automaton created in the early 1800s.[24]\"\n",
    "\n",
    "question = \"from when comes artificial intelligence ?\"\n",
    "output = pipe({\"context\": context, \"question\": question })\n",
    "print(question, \" - \", output)\n",
    "question = \"what is the concept behind artificial intelligence ?\"\n",
    "output = pipe({\"context\": context, \"question\": question })\n",
    "print(question, \" - \", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
